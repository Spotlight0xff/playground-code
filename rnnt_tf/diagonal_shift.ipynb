{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import better_exchook\n",
    "better_exchook.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INF = -float(\"inf\")\n",
    "\n",
    "def logsumexp(*args):  # summation in linear space -> LSE in log-space\n",
    "    \"\"\"\n",
    "    Stable log sum exp.\n",
    "    \"\"\"\n",
    "    if all(a == NEG_INF for a in args):\n",
    "        return NEG_INF\n",
    "    a_max = max(args)\n",
    "    lsp = np.log(sum(np.exp(a - a_max)\n",
    "                   for a in args))\n",
    "    return a_max + lsp\n",
    "\n",
    "\n",
    "def log_softmax(acts, axis=None):\n",
    "    \"\"\"computes log(softmax(x, axis)) in a numerical stable way.\"\"\"\n",
    "    assert axis is not None\n",
    "    a = acts - np.max(acts, axis=axis, keepdims=True)  # normalize for stability\n",
    "    probs = np.sum(np.exp(a), axis=axis, keepdims=True)\n",
    "    log_probs = a - np.log(probs)\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 3\n",
    "n_time = 5\n",
    "n_labels = 7\n",
    "n_vocab = 4\n",
    "np.random.seed(42)\n",
    "acts = np.random.random((n_batch, n_time, n_labels, n_vocab))\n",
    "labels = np.random.randint(1, n_vocab, (n_batch, n_labels-1))\n",
    "input_lengths = np.random.randint(1, n_time, (n_batch,), dtype=np.int32)\n",
    "label_lengths = np.random.randint(1, n_labels-1, (n_batch,), dtype=np.int32)\n",
    "log_probs = log_softmax(acts, axis=3)  # along vocabulary for (B, T, U, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to achieve:\n",
    "$$\n",
    "\\alpha(t,u) = \\bar{\\alpha}(u, t-u)\\\\\n",
    "\\beta(t,u) = \\bar{\\beta}(u, t-u)\\\\\n",
    "y(t, u, k) = \\bar{y}(u, t-u, k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rem_axes [<tf.Tensor: id=17, shape=(), dtype=int32, numpy=3>, <tf.Tensor: id=21, shape=(), dtype=int32, numpy=5>, <tf.Tensor: id=25, shape=(), dtype=int32, numpy=7>, <tf.Tensor: id=29, shape=(), dtype=int32, numpy=4>]\n",
      "rem_axes [<tf.Tensor: id=25, shape=(), dtype=int32, numpy=7>, <tf.Tensor: id=29, shape=(), dtype=int32, numpy=4>]\n",
      "[2, 3]\n",
      "reshaped (3, 5, 28)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c6bdaaaedeb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mshifted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriangular_shift_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_to_expand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mshifted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_time\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn_labels\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shifted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshifted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c6bdaaaedeb1>\u001b[0m in \u001b[0;36mtriangular_shift_matrix\u001b[0;34m(mat, axis, axis_to_expand, batch_axis)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#shifts = shifts[tf.newaxis, :, :, tf.newaxis]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#shifts = tf.tile(shifts, [n_batch, 1, 1, n_vocab])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mshifts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshifts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrem_axes_prod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, axis, *)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mpads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_axis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# (B, axis, *) ; (B, T, U, V) ; (B, T, T, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "def triangular_shift_matrix(mat, axis, axis_to_expand, batch_axis=0):\n",
    "    \"\"\"\n",
    "    Shifts the matrix in one dimension such that diagonal elements will be in one dimension.\n",
    "    \n",
    "    :param mat: matrix (B, ..., dim, ...)\n",
    "    :param axis: axis to perform the shifting\n",
    "    :param axis_from: axis from \n",
    "    :param int batch_axis:\n",
    "    :return Tensor of shape (B, ..., dim+dim_expand, ...)\n",
    "    \"\"\"\n",
    "    assert batch_axis == 0\n",
    "    # mat: (B, T, U, V)\n",
    "    # axis_to_expand: usually U\n",
    "    # axis: usually T\n",
    "    # batch-axis has to be first\n",
    "    dim_axis = tf.shape(mat)[axis]\n",
    "    n_batch = tf.shape(mat)[batch_axis]\n",
    "    #n_vocab = tf.shape(mat)[-1]\n",
    "    rem_axes = list(tf.shape(mat))\n",
    "    print(\"rem_axes\", rem_axes)\n",
    "    rem_axes.pop(axis)\n",
    "    rem_axes.pop(batch_axis)\n",
    "    print(\"rem_axes\", rem_axes)\n",
    "    #rem_axes.remove(axis)\n",
    "    #rem_axes.remove(batch_axis)\n",
    "    rem_axes_list = [i+2 for i in range(len(rem_axes))]\n",
    "    print(rem_axes_list)\n",
    "    mat = tf.transpose(mat, [0, axis, ] + rem_axes_list)  # (B, axis, ...)\n",
    "    rem_axes_prod = np.prod(rem_axes)\n",
    "    mat = tf.reshape(mat, (n_batch, dim_axis, rem_axes_prod))   # (B, axis, *)\n",
    "    print(\"reshaped\", mat.shape)\n",
    "    shifts = tf.cast(tf.range(dim_axis), tf.float32)  # (T,)\n",
    "    #shifts = shifts[tf.newaxis, :, :, tf.newaxis]\n",
    "    #shifts = tf.tile(shifts, [n_batch, 1, 1, n_vocab])\n",
    "    shifts = tf.tile(shifts[tf.newaxis, n, tf.newaxis], [n_batch, 1, rem_axes_prod])  # (B, axis, *)\n",
    "    pads = tf.zeros((n_batch, dim_axis,), dtype=tf.float32)\n",
    "    # (B, axis, *) ; (B, T, U, V) ; (B, T, T, V)\n",
    "    # -> (B, T, U+T+1, V)\n",
    "    a_ranged = tf.concat([shifts, tf.cast(mat, tf.float32), pads], axis=1)\n",
    "    #U = tf.shape(mat)[axis_to_expand]\n",
    "    #T = dim_axis\n",
    "    def fn(x): # x: (B, U+T+1, *)\n",
    "        shift = tf.cast(x[0][0][0], tf.int32) # (B,)\n",
    "        # 1:U+1 is the original data, in front: shift as wanted, back: padding for shape\n",
    "        n = tf.pad(x[:, 1:U+1, :], [[0,0],  # B\n",
    "                                      [shift, T+1-shift],  # U+T+1\n",
    "                                      [0,0] # V\n",
    "                                     ])\n",
    "        return n\n",
    "    t = tf.map_fn(fn, elems=tf.transpose(a_ranged, [1,0,2,3]))\n",
    "    t = tf.transpose(t, [1, 0, 2, 3])\n",
    "    return t\n",
    "shifted = triangular_shift_matrix(log_probs, axis=1, axis_to_expand=2)\n",
    "assert shifted.shape == (n_batch, n_time, n_time+n_labels+1, n_vocab)\n",
    "print(\"shifted\", shifted.shape)\n",
    "print_diagonal(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=4 0 4 -1.4747639375195227\n",
      "n=4 1 3 -1.7166377703348124\n",
      "n=4 2 2 -1.5795390703990098\n",
      "n=4 3 1 -1.0868037065231573\n",
      "n=4 4 0 -1.273270601638503\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'shifted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-70275a7a79d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint_diagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shifted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshifted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'shifted' is not defined"
     ]
    }
   ],
   "source": [
    "def print_diagonal(lp, n):\n",
    "    b = 0  # batch-idx\n",
    "    v = 0  # vocab-idx\n",
    "    # [0,2], [1,1], [2,0]\n",
    "    # [0,3], [1,2], [2,1], [3,0]\n",
    "    for i in range(0, n+1):\n",
    "        j = n - i\n",
    "        print(\"n=%d\" % n, i, j, lp[b, i, j, v])\n",
    "        #np.testing.assert_almost_equal(lp[b, i, j, v], shifted[b, i, n, v])\n",
    "n = 4\n",
    "print_diagonal(log_probs, n)\n",
    "print(\"shifted\", shifted[0, :, 4, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 3 1 1 2]\n",
      "idxs (3, 6, 5, 3)\n",
      "new shape [3, 6, 11]\n",
      "tf.Tensor(\n",
      "[[3 3 3 3 3 0 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 0 0 0 0 0]\n",
      " [0 0 3 3 3 3 3 0 0 0 0]\n",
      " [0 0 0 1 1 1 1 1 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 1 0 0]\n",
      " [0 0 0 0 0 2 2 2 2 2 0]], shape=(6, 11), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def shift_matrix_2d(mat, n_time, batch_dim_axis=0, axis=1, axis_to_shift=2):\n",
    "    assert batch_dim_axis == 0\n",
    "    mat = tf.convert_to_tensor(mat)\n",
    "    shape = tf.shape(mat)\n",
    "    mat = tf.expand_dims(mat, axis=-1)  # (B, U, 1)\n",
    "    mat = tf.tile(mat, [1,1, n_time])  # (B, U, T)\n",
    "    # batch, rows\n",
    "    B, R, C = tf.meshgrid(\n",
    "        tf.range(shape[0]),  # (B,)\n",
    "        tf.range(shape[1]),  # (U,)\n",
    "        tf.range(n_time)     # (T,)\n",
    "        ,indexing='ij')\n",
    "    shifts = tf.range(n_labels-1)  # (T,)\n",
    "    # (B, U, T) + (1, U, 1)\n",
    "    C = C + shifts[tf.newaxis, :, tf.newaxis]\n",
    "    idxs = tf.stack([B,R, C], axis=-1)\n",
    "    print(\"idxs\", idxs.shape)\n",
    "\n",
    "    # (B, U, T+U)\n",
    "    new_shape = [shape[0]]  # (B,)\n",
    "    new_shape.append(shape[1])\n",
    "    new_shape.append(shape[1] + n_time)\n",
    "    print(\"new shape\", [v.numpy() for v in new_shape])\n",
    "    # idxs: (B, U, U+T, 3)\n",
    "    scat_mat = tf.scatter_nd(indices=idxs, updates=mat,\n",
    "                            shape=new_shape)\n",
    "    return scat_mat\n",
    "\n",
    "print(labels[0])\n",
    "labels_shifted = shift_matrix_2d(labels, n_time=n_time)\n",
    "print(labels_shifted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_probs (3, 5, 7, 4)\n",
      "shape tf.Tensor([3 5 7 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6]\n",
      " [ 1  2  3  4  5  6  7]\n",
      " [ 2  3  4  5  6  7  8]\n",
      " [ 3  4  5  6  7  8  9]\n",
      " [ 4  5  6  7  8  9 10]], shape=(5, 7), dtype=int32)\n",
      "idxs (3, 5, 7, 3)\n",
      "new shape [3, 12, 7, 4]\n",
      "shifted (3, 12, 7, 4)\n",
      "expected (3, 12, 7, 4)\n",
      "n=4 0 4 -1.4747639375195227\n",
      "n=4 1 3 -1.7166377703348124\n",
      "n=4 2 2 -1.5795390703990098\n",
      "n=4 3 1 -1.0868037065231573\n",
      "n=4 4 0 -1.273270601638503\n",
      "shifted tf.Tensor(\n",
      "[-1.47476394 -1.71663777 -1.57953907 -1.08680371 -1.2732706   0.\n",
      "  0.          0.          0.          0.          0.          0.        ], shape=(12,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "def shift_rows(mat, shifts, batch_dim_axis=0, axis=1, axis_to_shift=2):\n",
    "    assert batch_dim_axis == 0\n",
    "    assert len(shifts.shape) == 1  # per row\n",
    "    from TFUtil import move_axis\n",
    "    mat = tf.convert_to_tensor(mat)\n",
    "    #mat = move_axis(mat, old_axis=axis_to_shift, new_axis=1)  # (B, axis, ...)\n",
    "    shape = tf.shape(mat)\n",
    "    print(\"shape\", shape)\n",
    "    #idxs_range = tf.stack([\n",
    "    #    tf.range(shape[0]),  # (B,)\n",
    "    #    tf.range(shape[1]),  # (T,)\n",
    "    #], axis=-1)  # (B, T, 2)\n",
    "    # batch, rows, cols\n",
    "    B, R, C = tf.meshgrid(\n",
    "        tf.range(shape[0]),  # (B,)\n",
    "        tf.range(shape[1]),  # (T,)\n",
    "        tf.range(shape[2])  # (U,)\n",
    "        ,indexing='ij')\n",
    "    # [B=3,T=5,U=7] + [1,5,1]\n",
    "    C = C + shifts[tf.newaxis, :, tf.newaxis]\n",
    "    idxs = tf.stack([B,R,C], axis=-1)\n",
    "    print(C[0])\n",
    "    # idxs are slices into the `mat` matrix\n",
    "    #print(idxs)\n",
    "    rem_shape = shape[2:] if len(shape) > 2 else []\n",
    "    print(\"idxs\", idxs.shape)\n",
    "\n",
    "    new_shape = [shape[0]]  # (B,)\n",
    "    new_shape.append(shape[1] + shape[2])\n",
    "    #new_shape.append(shape[2] + len(shifts))\n",
    "    new_shape.extend(rem_shape)  # (B, axis-to-shift, axis, ...)\n",
    "    print(\"new shape\", [v.numpy() for v in new_shape])\n",
    "    #mat_tr = tf.transpose(mat, (0, 2, 1, 3))  # (B, U, T, V)\n",
    "    # idxs: (B, U+T, 2)\n",
    "    scat_mat = tf.scatter_nd(indices=idxs, updates=mat,\n",
    "                            shape=new_shape)\n",
    "    \n",
    "    #shifted_idxs = tf.transpose(idxs[:,:,1] + shifts)\n",
    "    return scat_mat\n",
    "# B=3, T=5, U=7, V=4\n",
    "shifts = tf.range(n_time)  # (U,)\n",
    "print(\"log_probs\", log_probs.shape)\n",
    "shifted_mat = shift_rows(log_probs, shifts, axis=1, axis_to_shift=2)\n",
    "print(\"shifted\", shifted_mat.shape)\n",
    "print(\"expected\", (n_batch, n_time+n_labels, n_labels, n_vocab))\n",
    "#assert shifted_mat.shape == (n_batch, n_time+n_labels, n_labels, n_vocab)\n",
    "\n",
    "print_diagonal(log_probs, n=4)\n",
    "print(\"shifted\", shifted_mat[0, :, 4, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (returnn)",
   "language": "python",
   "name": "tf-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
