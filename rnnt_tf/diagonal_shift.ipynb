{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import better_exchook\n",
    "better_exchook.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INF = -float(\"inf\")\n",
    "\n",
    "def logsumexp(*args):  # summation in linear space -> LSE in log-space\n",
    "    \"\"\"\n",
    "    Stable log sum exp.\n",
    "    \"\"\"\n",
    "    if all(a == NEG_INF for a in args):\n",
    "        return NEG_INF\n",
    "    a_max = max(args)\n",
    "    lsp = np.log(sum(np.exp(a - a_max)\n",
    "                   for a in args))\n",
    "    return a_max + lsp\n",
    "\n",
    "\n",
    "def log_softmax(acts, axis=None):\n",
    "    \"\"\"computes log(softmax(x, axis)) in a numerical stable way.\"\"\"\n",
    "    assert axis is not None\n",
    "    a = acts - np.max(acts, axis=axis, keepdims=True)  # normalize for stability\n",
    "    probs = np.sum(np.exp(a), axis=axis, keepdims=True)\n",
    "    log_probs = a - np.log(probs)\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 3\n",
    "n_time = 5\n",
    "n_labels = 7\n",
    "n_vocab = 4\n",
    "np.random.seed(42)\n",
    "acts = np.random.random((n_batch, n_time, n_labels, n_vocab))\n",
    "labels = np.random.randint(1, n_vocab, (n_batch, n_labels-1))\n",
    "input_lengths = np.random.randint(1, n_time, (n_batch,), dtype=np.int32)\n",
    "label_lengths = np.random.randint(1, n_labels-1, (n_batch,), dtype=np.int32)\n",
    "log_probs = log_softmax(acts, axis=3)  # along vocabulary for (B, T, U, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to achieve:\n",
    "$$\n",
    "\\alpha(t,u) = \\bar{\\alpha}(u, t-u)\\\\\n",
    "\\beta(t,u) = \\bar{\\beta}(u, t-u)\\\\\n",
    "y(t, u, k) = \\bar{y}(u, t-u, k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_y (3, 5, 7, 4)\n"
     ]
    }
   ],
   "source": [
    "# log_probs: (B, T, U, V)\n",
    "new_y = tf.transpose(new_y, [0,2,1,3])  # (B, U, T, K)\n",
    "new_y = tf.roll(new_y, shift=[-n_labels], axis=[2])\n",
    "# this will break when we have uneven seq-lengths\n",
    "\n",
    "print(\"new_y\", new_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-1.007450722532022, shape=(), dtype=float64)\n",
      "-1.4083998261235249\n"
     ]
    }
   ],
   "source": [
    "u = 2\n",
    "t = 3\n",
    "i = 1  # batch idx\n",
    "k = 1\n",
    "print(new_y[i, u, t-u, k])\n",
    "print(log_probs[i, t, u, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rem_axes [<tf.Tensor: id=9504, shape=(), dtype=int32, numpy=3>, <tf.Tensor: id=9508, shape=(), dtype=int32, numpy=5>, <tf.Tensor: id=9512, shape=(), dtype=int32, numpy=7>, <tf.Tensor: id=9516, shape=(), dtype=int32, numpy=4>]\n",
      "rem_axes [<tf.Tensor: id=9512, shape=(), dtype=int32, numpy=7>, <tf.Tensor: id=9516, shape=(), dtype=int32, numpy=4>]\n",
      "[2, 3]\n",
      "reshaped (3, 5, 28)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected multiples argument to be a vector of length 2 but got length 3 [Op:Tile]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-885b564261d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mshifted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriangular_shift_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_to_expand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mshifted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_time\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn_labels\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shifted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshifted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-885b564261d4>\u001b[0m in \u001b[0;36mtriangular_shift_matrix\u001b[0;34m(mat, axis, axis_to_expand, batch_axis)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#shifts = shifts[tf.newaxis, :, :, tf.newaxis]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#shifts = tf.tile(shifts, [n_batch, 1, 1, n_vocab])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mshifts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshifts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrem_axes_prod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, axis, *)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mpads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_axis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# (B, T, 1, V) ; (B, T, U, V) ; (B, T, T, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/returnn/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(input, multiples, name)\u001b[0m\n\u001b[1;32m  10883\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10884\u001b[0m         return tile_eager_fallback(\n\u001b[0;32m> 10885\u001b[0;31m             input, multiples, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m  10886\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10887\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/returnn/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mtile_eager_fallback\u001b[0;34m(input, multiples, name, ctx)\u001b[0m\n\u001b[1;32m  10933\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tmultiples\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tmultiples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10934\u001b[0m   _result = _execute.execute(b\"Tile\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m> 10935\u001b[0;31m                              ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m  10936\u001b[0m   _execute.record_gradient(\n\u001b[1;32m  10937\u001b[0m       \"Tile\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m~/.virtualenvs/returnn/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/returnn/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected multiples argument to be a vector of length 2 but got length 3 [Op:Tile]"
     ]
    }
   ],
   "source": [
    "def triangular_shift_matrix(mat, axis, axis_to_expand, batch_axis=0):\n",
    "    \"\"\"\n",
    "    Shifts the matrix in one dimension such that diagonal elements will be in one dimension.\n",
    "    \n",
    "    :param mat: matrix (B, ..., dim, ...)\n",
    "    :param axis: axis to perform the shifting\n",
    "    :param axis_from: axis from \n",
    "    :param int batch_axis:\n",
    "    :return Tensor of shape (B, ..., dim+dim_expand, ...)\n",
    "    \"\"\"\n",
    "    assert batch_axis == 0\n",
    "    # mat: (B, T, U, V)\n",
    "    # axis_to_expand: usually U\n",
    "    # axis: usually T\n",
    "    # batch-axis has to be first\n",
    "    dim_axis = tf.shape(mat)[axis]\n",
    "    n_batch = tf.shape(mat)[batch_axis]\n",
    "    #n_vocab = tf.shape(mat)[-1]\n",
    "    rem_axes = list(tf.shape(mat))\n",
    "    print(\"rem_axes\", rem_axes)\n",
    "    rem_axes.pop(axis)\n",
    "    rem_axes.pop(batch_axis)\n",
    "    print(\"rem_axes\", rem_axes)\n",
    "    #rem_axes.remove(axis)\n",
    "    #rem_axes.remove(batch_axis)\n",
    "    rem_axes_list = [i+2 for i in range(len(rem_axes))]\n",
    "    print(rem_axes_list)\n",
    "    mat = tf.transpose(mat, [0, axis, ] + rem_axes_list)  # (B, axis, ...)\n",
    "    rem_axes_prod = np.prod(rem_axes)\n",
    "    mat = tf.reshape(mat, (n_batch, dim_axis, rem_axes_prod))   # (B, axis, *)\n",
    "    print(\"reshaped\", mat.shape)\n",
    "    shifts = tf.cast(tf.range(dim_axis), tf.float32)  # (T,)\n",
    "    #shifts = shifts[tf.newaxis, :, :, tf.newaxis]\n",
    "    #shifts = tf.tile(shifts, [n_batch, 1, 1, n_vocab])\n",
    "    shifts = tf.tile(shifts[tf.newaxis, n, tf.newaxis], [n_batch, 1, rem_axes_prod])  # (B, axis, *)\n",
    "    pads = tf.zeros((n_batch, dim_axis,), dtype=tf.float32)\n",
    "    # (B, axis, *) ; (B, T, U, V) ; (B, T, T, V)\n",
    "    # -> (B, T, U+T+1, V)\n",
    "    a_ranged = tf.concat([shifts, tf.cast(mat, tf.float32), pads], axis=1)\n",
    "    #U = tf.shape(mat)[axis_to_expand]\n",
    "    #T = dim_axis\n",
    "    def fn(x): # x: (B, U+T+1, *)\n",
    "        shift = tf.cast(x[0][0][0], tf.int32) # (B,)\n",
    "        # 1:U+1 is the original data, in front: shift as wanted, back: padding for shape\n",
    "        n = tf.pad(x[:, 1:U+1, :], [[0,0],  # B\n",
    "                                      [shift, T+1-shift],  # U+T+1\n",
    "                                      [0,0] # V\n",
    "                                     ])\n",
    "        return n\n",
    "    t = tf.map_fn(fn, elems=tf.transpose(a_ranged, [1,0,2,3]))\n",
    "    t = tf.transpose(t, [1, 0, 2, 3])\n",
    "    return t\n",
    "shifted = triangular_shift_matrix(log_probs, axis=1, axis_to_expand=2)\n",
    "assert shifted.shape == (n_batch, n_time, n_time+n_labels+1, n_vocab)\n",
    "print(\"shifted\", shifted.shape)\n",
    "print_diagonal(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=4 0 4 -1.4747639375195227 0.000\n",
      "n=4 1 3 -1.7166377703348124 -0.000\n",
      "n=4 2 2 -1.5795390703990098 -0.000\n",
      "n=4 3 1 -1.0868037065231573 -0.000\n",
      "n=4 4 0 -1.273270601638503 0.000\n",
      "shifted tf.Tensor([-1.474764  -1.7166377 -1.5795391 -1.0868037 -1.2732706], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def print_diagonal(lp):\n",
    "    b = 0  # batch-idx\n",
    "    v = 0  # vocab-idx\n",
    "    # [0,2], [1,1], [2,0]\n",
    "    # [0,3], [1,2], [2,1], [3,0]\n",
    "    for n in range(4,5):\n",
    "        for i in range(0, n+1):\n",
    "            j = n - i\n",
    "            print(\"n=%d\" % n, i, j, lp[b, i, j, v], \"%.3f\" % (lp[b, i, j, v] - shifted[b, i, n, v].numpy()))\n",
    "            np.testing.assert_almost_equal(lp[b, i, j, v], shifted[b, i, n, v])\n",
    "print_diagonal(log_probs)\n",
    "n = 4\n",
    "print(\"shifted\", shifted[0, :, 4, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 2. 3. 4.]\n",
      " [0. 1. 2. 3. 4.]\n",
      " [0. 1. 2. 3. 4.]], shape=(3, 5), dtype=float32)\n",
      "x tf.Tensor([0. 1. 2. 3. 4. 3. 1. 3. 1. 1. 2.], shape=(11,), dtype=float32)\n",
      "original tf.Tensor([3. 1. 3. 1. 1. 2.], shape=(6,), dtype=float32)\n",
      "shift: tf.Tensor(0, shape=(), dtype=int32)\n",
      "x tf.Tensor([0. 1. 2. 3. 4. 2. 3. 2. 3. 2. 1.], shape=(11,), dtype=float32)\n",
      "original tf.Tensor([2. 3. 2. 3. 2. 1.], shape=(6,), dtype=float32)\n",
      "shift: tf.Tensor(0, shape=(), dtype=int32)\n",
      "x tf.Tensor([0. 1. 2. 3. 4. 3. 3. 1. 3. 3. 2.], shape=(11,), dtype=float32)\n",
      "original tf.Tensor([3. 3. 1. 3. 3. 2.], shape=(6,), dtype=float32)\n",
      "shift: tf.Tensor(0, shape=(), dtype=int32)\n",
      "(3, 6)\n",
      "tf.Tensor(\n",
      "[[3. 1. 3. 1. 1. 2. 0. 0. 0. 0. 0. 0.]\n",
      " [2. 3. 2. 3. 2. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [3. 3. 1. 3. 3. 2. 0. 0. 0. 0. 0. 0.]], shape=(3, 12), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def shift_matrix_2d(mat, n_time):\n",
    "    \"\"\"\n",
    "    mat: (B, U)\n",
    "    :param int n_time:\n",
    "    :param int axis \n",
    "    \"\"\"\n",
    "    n_batch = tf.shape(mat)[0]\n",
    "    U = tf.shape(mat)[1]\n",
    "    shifts = tf.cast(tf.range(n_time), tf.float32)  # (T,)\n",
    "    shifts = tf.tile(shifts[tf.newaxis, :], [n_batch, 1])  # (B, T)\n",
    "    #shifts = shifts[tf.newaxis, :]  # (1, T)\n",
    "    print(shifts)\n",
    "    pads = tf.zeros((n_batch, n_time), dtype=tf.float32)\n",
    "    # (B, T) ; (B, U)\n",
    "    # -> (B, 1+U+T)\n",
    "    a_ranged = tf.concat([shifts, tf.cast(mat, tf.float32)], axis=1)\n",
    "    #U = tf.shape(mat)[axis_to_expand]\n",
    "    #T = tf.shape(mat)[1]\n",
    "\n",
    "    def fn(x): # x: (B,)\n",
    "        print(\"x\", x)\n",
    "        print(\"original\", x[n_time:n_time+U])\n",
    "        shift = tf.cast(x[0], tf.int32) # scalar\n",
    "        print(\"shift:\", shift)\n",
    "        # 1:U+1 is the original data, in front: shift as wanted, back: padding for shape\n",
    "        n = tf.pad(x[n_time:n_time+U], [[shift, n_time+1-shift],  # U+T+1\n",
    "                                     ])\n",
    "        return n\n",
    "    t = tf.map_fn(fn, elems=a_ranged)\n",
    "    #t = tf.transpose(t, [1, 0])  # (B, U+T+1)\n",
    "    return t\n",
    "\n",
    "labels_shifted = shift_matrix_2d(labels, n_time=n_time)\n",
    "print(labels.shape)\n",
    "print(labels_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (returnn)",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
