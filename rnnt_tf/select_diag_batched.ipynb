{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import TFUtil\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INF = -float(\"inf\")\n",
    "\n",
    "def logsumexp(*args):  # summation in linear space -> LSE in log-space\n",
    "    \"\"\"\n",
    "    Stable log sum exp.\n",
    "    \"\"\"\n",
    "    if all(a == NEG_INF for a in args):\n",
    "        return NEG_INF\n",
    "    a_max = max(args)\n",
    "    lsp = np.log(sum(np.exp(a - a_max)\n",
    "                   for a in args))\n",
    "    return a_max + lsp\n",
    "\n",
    "\n",
    "def log_softmax(acts, axis=None):\n",
    "    \"\"\"computes log(softmax(x, axis)) in a numerical stable way.\"\"\"\n",
    "    assert axis is not None\n",
    "    a = acts - np.max(acts, axis=axis, keepdims=True)  # normalize for stability\n",
    "    probs = np.sum(np.exp(a), axis=axis, keepdims=True)\n",
    "    log_probs = a - np.log(probs)\n",
    "    return log_probs\n",
    "\n",
    "def py_print_iteration_info(msg, var, n, debug=True):\n",
    "    \"\"\"adds a tf.print op to the graph while ensuring it will run (when the output is used).\"\"\"\n",
    "    if not debug:\n",
    "        return var\n",
    "    var_print = tf.print(\"n=\", n, \"\\t\", msg, tf.shape(var), var, output_stream=sys.stdout)\n",
    "    with tf.control_dependencies([var_print]):\n",
    "        var = tf.identity(var)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = np.array(\n",
    "    [\n",
    "      [[[0.06535690384862791, 0.7875301411923206, 0.08159176605666074],\n",
    "        [0.5297155426466327, 0.7506749639230854, 0.7541348379087998],\n",
    "        [0.6097641124736383, 0.8681404965673826, 0.6225318186056529]],\n",
    "\n",
    "       [[0.6685222872103057, 0.8580392805336061, 0.16453892311765583],\n",
    "        [0.989779515236694, 0.944298460961015, 0.6031678586829663],\n",
    "        [0.9467833543605416, 0.666202507295747, 0.28688179752461884]],\n",
    "\n",
    "       [[0.09418426230195986, 0.3666735970751962, 0.736168049462793],\n",
    "        [0.1666804425271342, 0.7141542198635192, 0.3993997272216727],\n",
    "        [0.5359823524146038, 0.29182076440286386, 0.6126422611507932]],\n",
    "\n",
    "       [[0.3242405528768486, 0.8007644367291621, 0.5241057606558068],\n",
    "        [0.779194617063042, 0.18331417220174862, 0.113745182072432],\n",
    "        [0.24022162381327106, 0.3394695622533106, 0.1341595066017014]]],\n",
    "\n",
    "      [[[0.5055615569388828, 0.051597282072282646, 0.6402903936686337],\n",
    "        [0.43073311517251, 0.8294731834714112, 0.1774668847323424],\n",
    "        [0.3207001991262245, 0.04288308912457006, 0.30280282975568984]],\n",
    "\n",
    "       [[0.6751777088333762, 0.569537369330242, 0.5584738347504452],\n",
    "        [0.08313242153985256, 0.06016544344162322, 0.10795752845152584],\n",
    "        [0.7486153608562472, 0.943918041459349, 0.4863558118797222]],\n",
    "\n",
    "       [[0.4181986264486809, 0.6524078485043804, 0.024242983423721887],\n",
    "        [0.13458171554507403, 0.3663418070512402, 0.2958297395361563],\n",
    "        [0.9236695822497084, 0.6899291482654177, 0.7418981733448822]],\n",
    "\n",
    "       [[0.25000547599982104, 0.6034295486281007, 0.9872887878887768],\n",
    "        [0.5926057265215715, 0.8846724004467684, 0.5434495396894328],\n",
    "        [0.6607698886038497, 0.3771277082495921, 0.3580209022231813]]]])\n",
    "\n",
    "labels = np.array([[1, 2],\n",
    "                 [1, 1]])\n",
    "input_lengths = np.array([4, 3], dtype=np.int32)\n",
    "label_lengths = np.array([2, 2], dtype=np.int32)\n",
    "log_probs = log_softmax(acts, axis=3)  # along vocabulary for (B, T, U, V)\n",
    "n_batch = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-d10eb122bc3a>:47: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "len_b tf.Tensor([2 2], shape=(2,), dtype=int32)\n",
      "mask_b tf.Tensor(\n",
      "[[ True  True False]\n",
      " [ True  True False]], shape=(2, 3), dtype=bool)\n",
      "range_mat tf.Tensor(\n",
      "[[0 1 2]\n",
      " [0 1 2]], shape=(2, 3), dtype=int32)\n",
      "start_c tf.Tensor([1 1], shape=(2,), dtype=int32) len(b): tf.Tensor([3 3], shape=(2,), dtype=int32)\n",
      "mask_c tf.Tensor(\n",
      "[[False  True  True]\n",
      " [False  True  True]], shape=(2, 3), dtype=bool)\n",
      "indices pre-mask (2, 3, 3) tf.Tensor(\n",
      "[[[0 0 2]\n",
      "  [0 1 1]\n",
      "  [0 2 0]]\n",
      "\n",
      " [[1 0 2]\n",
      "  [1 1 1]\n",
      "  [1 2 0]]], shape=(2, 3, 3), dtype=int32)\n",
      "indices post-mask (2, 3) tf.Tensor(\n",
      "[[0 1 1]\n",
      " [1 1 1]], shape=(2, 3), dtype=int32)\n",
      "indices post-mask-reshape (2, 1, 3) tf.Tensor(\n",
      "[[[0 1 1]]\n",
      "\n",
      " [[1 1 1]]], shape=(2, 1, 3), dtype=int32)\n",
      "gather from (2, 4, 3)\n",
      "idxs (2, 1, 3) tf.Tensor(\n",
      "[[[0 1 1]]\n",
      "\n",
      " [[1 1 1]]], shape=(2, 1, 3), dtype=int32)\n",
      "lp_blank (2, 1)\n"
     ]
    }
   ],
   "source": [
    "def select_diagonal_batched(n=0, input_lens=None, label_lens=None):\n",
    "    \"\"\"\n",
    "    Helper function to index various diagonals in a 2D matrix, which can be non-square.\n",
    "    One diagonal starts from the top-right and goes down to the bottom-left.\n",
    "    `n=1` indices (with start_row=0, start_col=0):\n",
    "    [[0,0]]\n",
    "    `n`=2:\n",
    "    [[0,1], [1,0]]\n",
    "    `n`=3:\n",
    "    [[0,2], [1,1], [2,0]]\n",
    "\n",
    "    :param n: specifies the diagonal to select\n",
    "    :param tf.Tensor input_lens:\n",
    "    :param tf.Tensor label_lens:\n",
    "    :return: (B, N') tensor of indices\n",
    "    :rtype: tf.Tensor\n",
    "    \"\"\"\n",
    "    from TFUtil import expand_dims_unbroadcast, sequence_mask\n",
    "    n_tiled = tf.tile([n], [n_batch])  # (B,)\n",
    "    diff_t_u = tf.abs(input_lens - label_lens)  # (B,)\n",
    "    min_t_u = tf.minimum(input_lens, label_lens)  # (B,)\n",
    "    max_t_u = tf.maximum(input_lens, label_lens)  # (B,)\n",
    "    \n",
    "    # diagonal lengths\n",
    "    #n_prime_a = tf.where(tf.less_equal(n_tiled, min_t_u), n_tiled, tf.zeros_like(n_tiled)-1)\n",
    "    #n_prime_b = tf.where(tf.greater(n_tiled, min_t_u), min_t_u, tf.zeros_like(n_tiled)-1)\n",
    "    #n_prime_c = tf.where(tf.greater(n_tiled, min_t_u + diff_t_u), min_t_u + diff_t_u - n_tiled, tf.zeros_like(n_tiled)-1)\n",
    "    #diag_len = tf.reduce_max(tf.stack([n_prime_a, n_prime_b, n_prime_c]), axis=0)  # (B,)\n",
    "    #diag_len = py_print_iteration_info(\"diag len\", diag_len, n, debug=True)\n",
    "    \n",
    "\n",
    "    batch_idxs = expand_dims_unbroadcast(tf.range(n_batch), 1, n)  # (B, N)\n",
    "    batch_idxs = tf.reshape(batch_idxs, (-1,))  # (B*N,)\n",
    "    indices = tf.stack([\n",
    "        batch_idxs,\n",
    "        tf.tile(tf.range(0, n), [n_batch]),\n",
    "        tf.tile(n - tf.range(n) - 1, [n_batch]),\n",
    "    ], axis=-1)  # (N*B, 3)\n",
    "    \n",
    "    # reshape, so that we have for each batch each item in the diag\n",
    "    indices = tf.reshape(indices, [n_batch, n, 3])  # (B, N, 3)\n",
    "    \n",
    "    # mask for phase (b)\n",
    "    idxs_len_b = tf.where(tf.logical_and(\n",
    "        tf.greater(n_tiled, min_t_u),\n",
    "        tf.less_equal(n_tiled, min_t_u + diff_t_u)),\n",
    "                           min_t_u, n_tiled)\n",
    "    print(\"len_b\", idxs_len_b)\n",
    "    idxs_mask_b = tf.where(input_lens > label_lens,\n",
    "                           tf.sequence_mask(idxs_len_b, maxlen=n),  # T > U\n",
    "                           tf.reverse(tf.sequence_mask(idxs_len_b, maxlen=n), axis=[0]) # U > T\n",
    "                          )  # (B, N)\n",
    "    print(\"mask_b\", idxs_mask_b)\n",
    "    \n",
    "    # mask for phase (c)\n",
    "    idxs_len_c = tf.where(tf.greater(n_tiled, min_t_u + diff_t_u),\n",
    "                         n_tiled - (min_t_u + diff_t_u),  # phase (c)\n",
    "                          n_tiled)  # default-case\n",
    "    idxs_start_c = tf.where(tf.greater(n_tiled, min_t_u + diff_t_u),\n",
    "                            min_t_u + diff_t_u,\n",
    "                            tf.ones_like(n_tiled))  # (B,)\n",
    "    # build mask from slice limits\n",
    "    range_mat = tf.expand_dims(tf.tile([0], [n_batch]), axis=1) \\\n",
    "    + tf.expand_dims(tf.range(n), axis=0)\n",
    "    print(\"range_mat\", range_mat)\n",
    "    idxs_mask_c = tf.where(tf.logical_and(range_mat >= tf.expand_dims(idxs_start_c, axis=1), # (B, 1)\n",
    "                                          range_mat < tf.expand_dims(idxs_start_c+idxs_len_c, axis=1)),\n",
    "                       tf.ones_like(range_mat),\n",
    "                       tf.zeros_like(range_mat)\n",
    "                      )  # (B, N)\n",
    "    idxs_mask_c = tf.cast(idxs_mask_c, tf.bool)\n",
    "    print(\"start_c\", idxs_start_c, \"len(b):\", idxs_len_c)\n",
    "    print(\"mask_c\", idxs_mask_c)\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"indices pre-mask\", indices.shape, indices)\n",
    "    mask = tf.logical_and(idxs_mask_b, idxs_mask_c)\n",
    "    idxs = tf.boolean_mask(indices, mask)\n",
    "    print(\"indices post-mask\", idxs.shape, idxs)\n",
    "    \n",
    "    idxs = tf.reshape(idxs, [n_batch, -1, 3])\n",
    "    \n",
    "    print(\"indices post-mask-reshape\", idxs.shape, idxs)\n",
    "    return idxs\n",
    "idxs = select_diagonal_batched(n=3, input_lens=input_lengths, label_lens=label_lengths)\n",
    "# (B*N, 3)\n",
    "# -> (B, N, 3)\n",
    "#idxs = tf.reshape(idxs, (n_batch, -1, 3))\n",
    "#idxs = idxs[:, :-1, :]\n",
    "#idxs = tf.reshape(idxs, (-1, 3))\n",
    "#idxs = idxs[:, :-1]\n",
    " # (B=2, T=4, U=3, V=3)\n",
    "print(\"gather from\", log_probs[:, :, :, 0].shape)\n",
    "print(\"idxs\", idxs.shape, idxs)  # (B, N, 2)\n",
    "# gather: data=(2, 4, 3) using idxs=(2,1,2)\n",
    "lp_blank = tf.gather_nd(log_probs[:, :, :, 0], idxs)\n",
    "lp_blank = tf.reshape(lp_blank, (n_batch, -1))\n",
    "print(\"lp_blank\", lp_blank.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U=3, T=3, V=3\n",
      "t= 1 u= 1: LSE(-1.476 + -1.184, -1.022 +  -1.132) = LSE(-2.660, -2.154) = -1.682\n",
      "t= 1 u= 2: LSE(-2.261 + -1.008, -1.682 +  -1.122) = LSE(-3.269, -2.804) = -2.317\n",
      "t= 2 u= 1: LSE(-1.682 + -1.099, -2.048 +  -0.844) = LSE(-2.781, -2.892) = -2.142\n",
      "t= 2 u= 2: LSE(-2.317 + -1.094, -2.142 +  -1.002) = LSE(-3.410, -3.144) = -2.575\n",
      "Alpha matrix: (3, 3)\n",
      "[[ 0.         -1.47617485 -2.2610643 ]\n",
      " [-1.02221057 -1.68195323 -2.31674093]\n",
      " [-2.04810766 -2.14188269 -2.57539042]]\n",
      "log-posterior = alpha[2, 2] + log_probs[2, 2, 0] = -2.575 + -0.965 = -3.5406\n",
      "alpha\n",
      "[[ 0.         -1.47617485 -2.2610643 ]\n",
      " [-1.02221057 -1.68195323 -2.31674093]\n",
      " [-2.04810766 -2.14188269 -2.57539042]]\n",
      "\n",
      "ll_forward\n",
      "-3.5406081372922227\n"
     ]
    }
   ],
   "source": [
    "from ref_transduce import forward_pass\n",
    "from rnnt_tf_impl import numpy_forward as forward_pass_debug\n",
    "i = 1\n",
    "alphas, ll_forward = forward_pass_debug(log_probs[i][:input_lengths[i], :label_lengths[i]+1], labels[i], blank_index=0, debug=True)\n",
    "alphas, ll_forward = forward_pass(log_probs[i][:input_lengths[i], :label_lengths[i]+1], labels[i], blank=0)\n",
    "print(\"alpha\")\n",
    "print(alphas)\n",
    "print(\"\\nll_forward\")\n",
    "print(ll_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=143, shape=(2, 3), dtype=float64, numpy=\n",
       "array([[-1.40493705, -2.43911218, -3.87740021],\n",
       "       [-1.02221057, -2.04810766, -3.12593642]])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for precomputation_col\n",
    "tf.cumsum(log_probs[:, :, 0, 0], exclusive=False, axis=1)[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0 0 1]\n",
      "  [0 1 2]]\n",
      "\n",
      " [[1 0 1]\n",
      "  [1 1 1]]], shape=(2, 2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# for precomputation_row\n",
    "n_target = tf.reduce_max(label_lengths+1)\n",
    "from TFUtil import expand_dims_unbroadcast\n",
    "a = expand_dims_unbroadcast(tf.range(n_batch), axis=1, dim=n_target-1)  # (B,U-1)\n",
    "b = expand_dims_unbroadcast(tf.range(n_target - 1), axis=0, dim=n_batch) # (B, U-1)\n",
    "c = labels # (B, U-1)\n",
    "indices_w_labels = tf.stack([a, b, c], axis=-1)   # (B, U-1, 3)\n",
    "print(indices_w_labels)\n",
    "# log_probs[:,0,:,:]: (B, U, V)\n",
    "log_probs_y = tf.gather_nd(log_probs[:,0,:,:], indices_w_labels)\n",
    "# -> (B, U-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=176, shape=(2, 2), dtype=float64, numpy=\n",
       "array([[-0.68276381, -1.71078415],\n",
       "       [-1.47617485, -2.2610643 ]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cumsum(log_probs_y, exclusive=False, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=184, shape=(2, 3), dtype=float64, numpy=\n",
       "array([[ 0.        , -0.68276381, -1.71078415],\n",
       "       [ 0.        , -1.47617485, -2.2610643 ]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([tf.cast(tf.tile([[0.]], [n_batch,1]), tf.double), tf.cumsum(log_probs_y, exclusive=False, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=207, shape=(3,), dtype=int64, numpy=array([2, 0, 1])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.constant([\n",
    "    [0  ,   0,   1,   0, 1],\n",
    "    [1,   0,   1, 1,   0],\n",
    "    [0  , 1,     0,   0,   0]])\n",
    "\n",
    "tmp_indices = tf.where(m)\n",
    "tf.segment_min(tmp_indices[:, 1], tmp_indices[:, 0])\n",
    "#tf.argmin(, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 3, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B=2, T=4, U=2, V=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 20\n",
    "n_vocab = 5\n",
    "max_target = 4\n",
    "max_input = 8\n",
    "np.random.seed(42)\n",
    "labels = np.random.randint(1, n_vocab, (n_batch, max_target-1))\n",
    "input_lengths = np.random.randint(1, max_input, (n_batch,), dtype=np.int32)\n",
    "label_lengths = np.random.randint(1, max_target, (n_batch,), dtype=np.int32)\n",
    "acts = np.random.normal(0, 1, (n_batch, max_input, max_target, n_vocab))\n",
    "log_probs = log_softmax(acts, axis=3)  # along vocabulary for (B, T, U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U=2, T=2, V=5\n",
      "t= 1 u= 1: LSE(-1.810 + -1.891, -1.534 +  -1.054) = LSE(-3.701, -2.587) = -2.303\n",
      "Alpha matrix: (2, 2)\n",
      "[[ 0.         -1.80972908]\n",
      " [-1.53368601 -2.30339965]]\n",
      "log-posterior = alpha[1, 1] + log_probs[1, 1, 0] = -2.303 + -1.192 = -3.4954\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "alphas, ll_forward = forward_pass_debug(log_probs[i][:input_lengths[i], :label_lengths[i]+1], labels[i], blank_index=0, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (returnn)",
   "language": "python",
   "name": "tf-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
